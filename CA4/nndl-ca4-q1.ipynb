{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1238650,"sourceType":"datasetVersion","datasetId":710083}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install hazm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:33:56.136649Z","iopub.execute_input":"2024-12-18T07:33:56.137011Z","iopub.status.idle":"2024-12-18T07:34:20.091816Z","shell.execute_reply.started":"2024-12-18T07:33:56.136969Z","shell.execute_reply":"2024-12-18T07:34:20.090947Z"}},"outputs":[{"name":"stdout","text":"Collecting hazm\n  Downloading hazm-0.10.0-py3-none-any.whl.metadata (11 kB)\nCollecting fasttext-wheel<0.10.0,>=0.9.2 (from hazm)\n  Downloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nCollecting flashtext<3.0,>=2.7 (from hazm)\n  Downloading flashtext-2.7.tar.gz (14 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: gensim<5.0.0,>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from hazm) (4.3.3)\nCollecting nltk<4.0.0,>=3.8.1 (from hazm)\n  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\nCollecting numpy==1.24.3 (from hazm)\n  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nCollecting python-crfsuite<0.10.0,>=0.9.9 (from hazm)\n  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /opt/conda/lib/python3.10/site-packages (from hazm) (1.2.2)\nRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.10/site-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.13.6)\nRequirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (70.0.0)\nCollecting scipy<1.14.0,>=1.7.0 (from gensim<5.0.0,>=4.3.1->hazm)\n  Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.0.4)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (2024.5.15)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.5.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.16.0)\nDownloading hazm-0.10.0-py3-none-any.whl (892 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m892.6/892.6 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fasttext_wheel-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: flashtext\n  Building wheel for flashtext (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9296 sha256=6409cfb8256647aea41277e645aa532e6eed565b58ce8d4a5905baf136572c7c\n  Stored in directory: /root/.cache/pip/wheels/bc/be/39/c37ad168eb2ff644c9685f52554440372129450f0b8ed203dd\nSuccessfully built flashtext\nInstalling collected packages: flashtext, python-crfsuite, numpy, nltk, scipy, fasttext-wheel, hazm\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.14.1\n    Uninstalling scipy-1.14.1:\n      Successfully uninstalled scipy-1.14.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.10.1 requires cubinlinker, which is not installed.\ncudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.10.1 requires libcudf==24.10.*, which is not installed.\ncudf 24.10.1 requires ptxcompiler, which is not installed.\ncuml 24.10.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 24.10.0 requires cuvs==24.10.*, which is not installed.\ncuml 24.10.0 requires nvidia-cublas, which is not installed.\ncuml 24.10.0 requires nvidia-cufft, which is not installed.\ncuml 24.10.0 requires nvidia-curand, which is not installed.\ncuml 24.10.0 requires nvidia-cusolver, which is not installed.\ncuml 24.10.0 requires nvidia-cusparse, which is not installed.\ndask-cudf 24.10.1 requires cupy-cuda11x>=12.0.0, which is not installed.\npylibcudf 24.10.1 requires libcudf==24.10.*, which is not installed.\npylibraft 24.10.0 requires nvidia-cublas, which is not installed.\npylibraft 24.10.0 requires nvidia-curand, which is not installed.\npylibraft 24.10.0 requires nvidia-cusolver, which is not installed.\npylibraft 24.10.0 requires nvidia-cusparse, which is not installed.\nucxx 0.40.0 requires libucxx==0.40.*, which is not installed.\nalbucore 0.0.20 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\nalbumentations 1.4.21 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.1.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\nbayesian-optimization 2.0.0 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\nblis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.24.3 which is incompatible.\ncudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\ncudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\ndask-cudf 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\nibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 17.0.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.3 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\npylibcudf 24.10.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\nrmm 24.10.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.2.post1 which is incompatible.\nthinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\ntsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\nxarray 2024.11.0 requires packaging>=23.2, but you have packaging 21.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fasttext-wheel-0.9.2 flashtext-2.7 hazm-0.10.0 nltk-3.9.1 numpy-1.24.3 python-crfsuite-0.9.11 scipy-1.13.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport re\nfrom hazm import Normalizer, stopwords_list, word_tokenize\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:34:20.094036Z","iopub.execute_input":"2024-12-18T07:34:20.094415Z","iopub.status.idle":"2024-12-18T07:34:31.932955Z","shell.execute_reply.started":"2024-12-18T07:34:20.094377Z","shell.execute_reply":"2024-12-18T07:34:31.932122Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:34:31.934002Z","iopub.execute_input":"2024-12-18T07:34:31.934571Z","iopub.status.idle":"2024-12-18T07:34:32.002369Z","shell.execute_reply.started":"2024-12-18T07:34:31.934512Z","shell.execute_reply":"2024-12-18T07:34:32.001428Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/persian-spam-email/emails.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:34:32.004688Z","iopub.execute_input":"2024-12-18T07:34:32.005123Z","iopub.status.idle":"2024-12-18T07:34:32.277006Z","shell.execute_reply.started":"2024-12-18T07:34:32.005073Z","shell.execute_reply":"2024-12-18T07:34:32.276335Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:34:32.278057Z","iopub.execute_input":"2024-12-18T07:34:32.278371Z","iopub.status.idle":"2024-12-18T07:34:32.285250Z","shell.execute_reply.started":"2024-12-18T07:34:32.278322Z","shell.execute_reply":"2024-12-18T07:34:32.284204Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(1000, 2)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:34:32.286154Z","iopub.execute_input":"2024-12-18T07:34:32.286534Z","iopub.status.idle":"2024-12-18T07:34:32.310549Z","shell.execute_reply.started":"2024-12-18T07:34:32.286494Z","shell.execute_reply":"2024-12-18T07:34:32.309147Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                text label\n0  ﻿ممنون آقا سامان.\\nمن پارسال اصلا آزاد شرکت نک...   ham\n1  ﻿سلام آقای کریمی\\nبالاخره آزمونارشد تموم شد من...   ham\n2  ﻿درود بر حاج وحیدی بنده بعنوان یک دکتری تاریخ ...   ham\n3  ﻿با سلام  و احترام\\nضمن تقدیر از مسولین محترم ...   ham\n4  ﻿با سلام اینجانب یک دستگاه خودرو پراید 131 با ...   ham","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>﻿ممنون آقا سامان.\\nمن پارسال اصلا آزاد شرکت نک...</td>\n      <td>ham</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>﻿سلام آقای کریمی\\nبالاخره آزمونارشد تموم شد من...</td>\n      <td>ham</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>﻿درود بر حاج وحیدی بنده بعنوان یک دکتری تاریخ ...</td>\n      <td>ham</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>﻿با سلام  و احترام\\nضمن تقدیر از مسولین محترم ...</td>\n      <td>ham</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>﻿با سلام اینجانب یک دستگاه خودرو پراید 131 با ...</td>\n      <td>ham</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"print(\"\\nLabel Distribution:\")\nprint(df['label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:34:32.311658Z","iopub.execute_input":"2024-12-18T07:34:32.312048Z","iopub.status.idle":"2024-12-18T07:34:32.321849Z","shell.execute_reply.started":"2024-12-18T07:34:32.312013Z","shell.execute_reply":"2024-12-18T07:34:32.320916Z"}},"outputs":[{"name":"stdout","text":"\nLabel Distribution:\nlabel\nham     500\nspam    500\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"sns.countplot(data=df, x='label', palette='viridis')\nplt.title(\"Distribution of Ham and Spam Messages\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:34:32.323027Z","iopub.execute_input":"2024-12-18T07:34:32.323866Z","iopub.status.idle":"2024-12-18T07:34:32.550248Z","shell.execute_reply.started":"2024-12-18T07:34:32.323810Z","shell.execute_reply":"2024-12-18T07:34:32.549410Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"normalizer = Normalizer()\nstopwords = set(stopwords_list())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:34:32.551220Z","iopub.execute_input":"2024-12-18T07:34:32.551491Z","iopub.status.idle":"2024-12-18T07:34:34.210848Z","shell.execute_reply.started":"2024-12-18T07:34:32.551464Z","shell.execute_reply":"2024-12-18T07:34:34.210104Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def preprocess_text(text):\n    # Normalize text\n    text = normalizer.normalize(text)\n    # Remove URLs\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    # Remove email addresses\n    text = re.sub(r'\\S+@\\S+\\.\\S+', '', text)\n    # Remove phone numbers (any sequence of 7+ digits)\n    text = re.sub(r'\\b\\d{7,}\\b', '', text)\n    # Reduce repeated characters (e.g., ععععلی → علی)\n    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n    # Tokenize and remove stopwords\n    tokens = word_tokenize(text)\n    tokens = [word for word in tokens if word not in stopwords]\n    return ' '.join(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:34:34.214786Z","iopub.execute_input":"2024-12-18T07:34:34.215047Z","iopub.status.idle":"2024-12-18T07:34:34.220376Z","shell.execute_reply.started":"2024-12-18T07:34:34.215022Z","shell.execute_reply":"2024-12-18T07:34:34.219562Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"df['cleaned_text'] = df['text'].apply(preprocess_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:34:34.221606Z","iopub.execute_input":"2024-12-18T07:34:34.221977Z","iopub.status.idle":"2024-12-18T07:34:37.846007Z","shell.execute_reply.started":"2024-12-18T07:34:34.221940Z","shell.execute_reply":"2024-12-18T07:34:37.845292Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:34:37.847010Z","iopub.execute_input":"2024-12-18T07:34:37.847370Z","iopub.status.idle":"2024-12-18T07:34:37.857923Z","shell.execute_reply.started":"2024-12-18T07:34:37.847331Z","shell.execute_reply":"2024-12-18T07:34:37.856883Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                                text label  \\\n0  ﻿ممنون آقا سامان.\\nمن پارسال اصلا آزاد شرکت نک...   ham   \n1  ﻿سلام آقای کریمی\\nبالاخره آزمونارشد تموم شد من...   ham   \n2  ﻿درود بر حاج وحیدی بنده بعنوان یک دکتری تاریخ ...   ham   \n3  ﻿با سلام  و احترام\\nضمن تقدیر از مسولین محترم ...   ham   \n4  ﻿با سلام اینجانب یک دستگاه خودرو پراید 131 با ...   ham   \n\n                                        cleaned_text  \n0  ﻿ممنون آقا سامان . پارسال اصلا آزاد شرکت نکرده...  \n1  ﻿سلام کریمی بالاخره آزمونارشد تموم راحت شدم یک...  \n2  ﻿درود حاج وحیدی بنده بعنوان دکتری تاریخ دستی ت...  \n3  ﻿با سلام احترام تقدیر مسولین محترم سایت تابناک...  \n4  ﻿با سلام اینجانب دستگاه خودرو پراید ۱۳۱ شماره ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>cleaned_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>﻿ممنون آقا سامان.\\nمن پارسال اصلا آزاد شرکت نک...</td>\n      <td>ham</td>\n      <td>﻿ممنون آقا سامان . پارسال اصلا آزاد شرکت نکرده...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>﻿سلام آقای کریمی\\nبالاخره آزمونارشد تموم شد من...</td>\n      <td>ham</td>\n      <td>﻿سلام کریمی بالاخره آزمونارشد تموم راحت شدم یک...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>﻿درود بر حاج وحیدی بنده بعنوان یک دکتری تاریخ ...</td>\n      <td>ham</td>\n      <td>﻿درود حاج وحیدی بنده بعنوان دکتری تاریخ دستی ت...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>﻿با سلام  و احترام\\nضمن تقدیر از مسولین محترم ...</td>\n      <td>ham</td>\n      <td>﻿با سلام احترام تقدیر مسولین محترم سایت تابناک...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>﻿با سلام اینجانب یک دستگاه خودرو پراید 131 با ...</td>\n      <td>ham</td>\n      <td>﻿با سلام اینجانب دستگاه خودرو پراید ۱۳۱ شماره ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"def tokenize_text(tokenizer, text, max_length=32):\n    tokens = tokenizer.encode_plus(\n        text,\n        max_length=max_length,\n        truncation=True,\n        padding='max_length',\n        return_tensors='pt'\n    )\n    return tokens['input_ids'].squeeze(0)\n\ndef get_embeddings(text, max_length=32):\n    tokens = tokenizer.encode_plus(\n        text,\n        max_length=max_length,\n        truncation=True,\n        padding='max_length',\n        return_tensors='pt'\n    )\n    input_ids = tokens['input_ids']\n    attention_mask = tokens['attention_mask']\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n    cls_embedding = outputs.last_hidden_state[:, 0, :]\n    return cls_embedding.squeeze(0).cpu().numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:35:43.871149Z","iopub.execute_input":"2024-12-18T07:35:43.872056Z","iopub.status.idle":"2024-12-18T07:35:43.878056Z","shell.execute_reply.started":"2024-12-18T07:35:43.872019Z","shell.execute_reply":"2024-12-18T07:35:43.877098Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\ndf['tokenized'] = df['cleaned_text'].apply(lambda x: tokenize_text(tokenizer, x).tolist())\n\nmodel = AutoModel.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\ndf['embeddings'] = df['cleaned_text'].apply(get_embeddings)\n\nembeddings = df['embeddings'].tolist()\npca = PCA(n_components=120)\nreduced_embeddings = pca.fit_transform(embeddings)\n\ndf['reduced_embeddings'] = list(reduced_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:35:45.307110Z","iopub.execute_input":"2024-12-18T07:35:45.307460Z","iopub.status.idle":"2024-12-18T07:36:54.181046Z","shell.execute_reply.started":"2024-12-18T07:35:45.307428Z","shell.execute_reply":"2024-12-18T07:36:54.179029Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"df.to_csv(\"preprocessed_data.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:36:59.326470Z","iopub.execute_input":"2024-12-18T07:36:59.327347Z","iopub.status.idle":"2024-12-18T07:37:04.396793Z","shell.execute_reply.started":"2024-12-18T07:36:59.327313Z","shell.execute_reply":"2024-12-18T07:37:04.395800Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/preprocessed_data.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:37:04.398745Z","iopub.execute_input":"2024-12-18T07:37:04.399638Z","iopub.status.idle":"2024-12-18T07:37:04.632599Z","shell.execute_reply.started":"2024-12-18T07:37:04.399593Z","shell.execute_reply":"2024-12-18T07:37:04.631878Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:37:04.633809Z","iopub.execute_input":"2024-12-18T07:37:04.634142Z","iopub.status.idle":"2024-12-18T07:37:04.646451Z","shell.execute_reply.started":"2024-12-18T07:37:04.634106Z","shell.execute_reply":"2024-12-18T07:37:04.645576Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                               text label  \\\n0           0  ﻿ممنون آقا سامان.\\nمن پارسال اصلا آزاد شرکت نک...   ham   \n1           1  ﻿سلام آقای کریمی\\nبالاخره آزمونارشد تموم شد من...   ham   \n2           2  ﻿درود بر حاج وحیدی بنده بعنوان یک دکتری تاریخ ...   ham   \n3           3  ﻿با سلام  و احترام\\nضمن تقدیر از مسولین محترم ...   ham   \n4           4  ﻿با سلام اینجانب یک دستگاه خودرو پراید 131 با ...   ham   \n\n                                        cleaned_text  \\\n0  ﻿ممنون آقا سامان . پارسال اصلا آزاد شرکت نکرده...   \n1  ﻿سلام کریمی بالاخره آزمونارشد تموم راحت شدم یک...   \n2  ﻿درود حاج وحیدی بنده بعنوان دکتری تاریخ دستی ت...   \n3  ﻿با سلام احترام تقدیر مسولین محترم سایت تابناک...   \n4  ﻿با سلام اینجانب دستگاه خودرو پراید ۱۳۱ شماره ...   \n\n                                           tokenized  \\\n0  [2, 25303, 7664, 9819, 1012, 14104, 5899, 3912...   \n1  [2, 4285, 10407, 7637, 6191, 42075, 31920, 630...   \n2  [2, 17960, 7343, 37605, 7912, 9355, 10042, 337...   \n3  [2, 2799, 4285, 6755, 8418, 45904, 8496, 4394,...   \n4  [2, 2799, 4285, 19622, 3698, 3379, 13814, 7088...   \n\n                                          embeddings  \\\n0  [ 2.03596130e-01 -6.73360646e-01  1.99428499e+...   \n1  [ 4.62497562e-01  6.67644739e-01  1.88398123e+...   \n2  [ 6.09418869e-01 -2.64802396e-01  1.13385665e+...   \n3  [ 1.05735135e+00 -4.83997107e-01  1.77630186e+...   \n4  [ 3.84518057e-01 -2.04942912e-01  1.79146230e+...   \n\n                                  reduced_embeddings  \n0  [ 1.22374149e+01 -2.53756607e+00 -4.00591262e+...  \n1  [ 7.17889295e+00 -2.17502630e+00  5.07199329e-...  \n2  [ 1.81639278 -2.74050864  7.66371104  1.086296...  \n3  [-0.328559   -6.2689549   9.89679616 -2.084088...  \n4  [ 4.47262368e+00 -2.48858125e+00  3.48697066e+...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>label</th>\n      <th>cleaned_text</th>\n      <th>tokenized</th>\n      <th>embeddings</th>\n      <th>reduced_embeddings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>﻿ممنون آقا سامان.\\nمن پارسال اصلا آزاد شرکت نک...</td>\n      <td>ham</td>\n      <td>﻿ممنون آقا سامان . پارسال اصلا آزاد شرکت نکرده...</td>\n      <td>[2, 25303, 7664, 9819, 1012, 14104, 5899, 3912...</td>\n      <td>[ 2.03596130e-01 -6.73360646e-01  1.99428499e+...</td>\n      <td>[ 1.22374149e+01 -2.53756607e+00 -4.00591262e+...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>﻿سلام آقای کریمی\\nبالاخره آزمونارشد تموم شد من...</td>\n      <td>ham</td>\n      <td>﻿سلام کریمی بالاخره آزمونارشد تموم راحت شدم یک...</td>\n      <td>[2, 4285, 10407, 7637, 6191, 42075, 31920, 630...</td>\n      <td>[ 4.62497562e-01  6.67644739e-01  1.88398123e+...</td>\n      <td>[ 7.17889295e+00 -2.17502630e+00  5.07199329e-...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>﻿درود بر حاج وحیدی بنده بعنوان یک دکتری تاریخ ...</td>\n      <td>ham</td>\n      <td>﻿درود حاج وحیدی بنده بعنوان دکتری تاریخ دستی ت...</td>\n      <td>[2, 17960, 7343, 37605, 7912, 9355, 10042, 337...</td>\n      <td>[ 6.09418869e-01 -2.64802396e-01  1.13385665e+...</td>\n      <td>[ 1.81639278 -2.74050864  7.66371104  1.086296...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>﻿با سلام  و احترام\\nضمن تقدیر از مسولین محترم ...</td>\n      <td>ham</td>\n      <td>﻿با سلام احترام تقدیر مسولین محترم سایت تابناک...</td>\n      <td>[2, 2799, 4285, 6755, 8418, 45904, 8496, 4394,...</td>\n      <td>[ 1.05735135e+00 -4.83997107e-01  1.77630186e+...</td>\n      <td>[-0.328559   -6.2689549   9.89679616 -2.084088...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>﻿با سلام اینجانب یک دستگاه خودرو پراید 131 با ...</td>\n      <td>ham</td>\n      <td>﻿با سلام اینجانب دستگاه خودرو پراید ۱۳۱ شماره ...</td>\n      <td>[2, 2799, 4285, 19622, 3698, 3379, 13814, 7088...</td>\n      <td>[ 3.84518057e-01 -2.04942912e-01  1.79146230e+...</td>\n      <td>[ 4.47262368e+00 -2.48858125e+00  3.48697066e+...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ndf['encoded_label'] = label_encoder.fit_transform(df['label'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:37:05.783395Z","iopub.execute_input":"2024-12-18T07:37:05.784238Z","iopub.status.idle":"2024-12-18T07:37:05.789316Z","shell.execute_reply.started":"2024-12-18T07:37:05.784202Z","shell.execute_reply":"2024-12-18T07:37:05.788306Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"print(\"Label Mapping:\", dict(zip(label_encoder.classes_, range(len(label_encoder.classes_)))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:37:07.001492Z","iopub.execute_input":"2024-12-18T07:37:07.002310Z","iopub.status.idle":"2024-12-18T07:37:07.006765Z","shell.execute_reply.started":"2024-12-18T07:37:07.002280Z","shell.execute_reply":"2024-12-18T07:37:07.005934Z"}},"outputs":[{"name":"stdout","text":"Label Mapping: {'ham': 0, 'spam': 1}\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"X = torch.tensor(reduced_embeddings, dtype=torch.float32)\ny = torch.tensor(df['encoded_label'].values, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:37:15.604167Z","iopub.execute_input":"2024-12-18T07:37:15.604979Z","iopub.status.idle":"2024-12-18T07:37:15.609681Z","shell.execute_reply.started":"2024-12-18T07:37:15.604943Z","shell.execute_reply":"2024-12-18T07:37:15.608737Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_val, y_val = X_val.to(device), y_val.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:37:16.809695Z","iopub.execute_input":"2024-12-18T07:37:16.810023Z","iopub.status.idle":"2024-12-18T07:37:16.817730Z","shell.execute_reply.started":"2024-12-18T07:37:16.809999Z","shell.execute_reply":"2024-12-18T07:37:16.816957Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"class CNNLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_classes):\n        super(CNNLSTM, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3)\n        self.lstm = nn.LSTM(16, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # Add channel dimension for CNN\n        x = self.conv1(x)\n        x = torch.relu(x)\n        x = x.permute(0, 2, 1)  # Prepare for LSTM\n        _, (hn, _) = self.lstm(x)\n        out = self.fc(hn[-1])\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:37:18.012349Z","iopub.execute_input":"2024-12-18T07:37:18.013299Z","iopub.status.idle":"2024-12-18T07:37:18.019640Z","shell.execute_reply.started":"2024-12-18T07:37:18.013252Z","shell.execute_reply":"2024-12-18T07:37:18.018777Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def train_model(batch_size, lr, optimizer_choice, num_epochs=50):\n    model = CNNLSTM(input_dim=120, hidden_dim=64, num_classes=len(y.unique())).to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optimizer_choice(model.parameters(), lr=lr)\n\n    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_val, y_val), batch_size=batch_size)\n\n    best_loss = float('inf')\n    best_model_state = None\n    \n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        \n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            \n        avg_train_loss = running_loss / len(train_loader)\n        \n        model.eval()\n        val_preds, val_true = [], []\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                val_preds.append(preds)\n                val_true.append(labels)\n\n        val_preds = torch.cat(val_preds).cpu().numpy()\n        val_true = torch.cat(val_true).cpu().numpy()\n        val_acc = accuracy_score(val_true, val_preds)\n        print(f\"    Epoch {epoch+1}, Train Loss: {avg_train_loss}, Validation Accuracy: {val_acc:.4f}\")\n\n        if avg_train_loss < best_loss:\n            best_loss = avg_train_loss\n            best_model_state = model.state_dict()\n    \n    return best_model_state\n\ndef evaluate_model(model, test_loader):\n    model.eval()\n    y_true, y_preds = [], []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            y_preds.extend(preds.cpu().numpy())\n            y_true.extend(labels.cpu().numpy())\n    \n    acc = accuracy_score(y_true, y_preds)\n    precision = precision_score(y_true, y_preds, average='macro')\n    recall = recall_score(y_true, y_preds, average='macro')\n    f1 = f1_score(y_true, y_preds, average='macro')\n    roc_auc = roc_auc_score(y_true, y_preds, multi_class='ovr')\n    \n    print(f\"    Accuracy: {acc:.4f}\")\n    print(f\"    Precision: {precision:.4f}\")\n    print(f\"    Recall: {recall:.4f}\")\n    print(f\"    F1-Score: {f1:.4f}\")\n    print(f\"    ROC AUC: {roc_auc:.4f}\")\n    \n    return f1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:49:59.169206Z","iopub.execute_input":"2024-12-18T07:49:59.170036Z","iopub.status.idle":"2024-12-18T07:49:59.182589Z","shell.execute_reply.started":"2024-12-18T07:49:59.170000Z","shell.execute_reply":"2024-12-18T07:49:59.181528Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test, y_test), batch_size=64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:49:56.937921Z","iopub.execute_input":"2024-12-18T07:49:56.938295Z","iopub.status.idle":"2024-12-18T07:49:56.943013Z","shell.execute_reply.started":"2024-12-18T07:49:56.938265Z","shell.execute_reply":"2024-12-18T07:49:56.942063Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"best_model_state = None\nbest_f1 = 0\n\nfor batch_size in batch_sizes:\n    for lr in learning_rates:\n        for opt in optimizers:\n            print(f\"Training with Batch={batch_size}, LR={lr}, Optimizer={opt.__name__}\")\n            model_state = train_model(batch_size, lr, opt)\n            model = CNNLSTM(input_dim=120, hidden_dim=64, num_classes=len(y.unique())).to(device)\n            model.load_state_dict(model_state)\n            print(\"\\nEvaluating on Test Data...\")\n            f1 = evaluate_model(model, test_loader)\n            if f1 > best_f1:\n                best_model_state = model.state_dict()\n                best_f1 = f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:50:18.224845Z","iopub.execute_input":"2024-12-18T07:50:18.225702Z","iopub.status.idle":"2024-12-18T07:50:58.084234Z","shell.execute_reply.started":"2024-12-18T07:50:18.225669Z","shell.execute_reply":"2024-12-18T07:50:58.083515Z"}},"outputs":[{"name":"stdout","text":"Training with Batch=8, LR=0.001, Optimizer=Adam\n    Epoch 1, Train Loss: 0.6955642197813307, Validation Accuracy: 0.4500\n    Epoch 2, Train Loss: 0.6929914798055377, Validation Accuracy: 0.4357\n    Epoch 3, Train Loss: 0.6919324721608843, Validation Accuracy: 0.4786\n    Epoch 4, Train Loss: 0.6912114220006126, Validation Accuracy: 0.4714\n    Epoch 5, Train Loss: 0.6898122319153377, Validation Accuracy: 0.4571\n    Epoch 6, Train Loss: 0.6884642745767321, Validation Accuracy: 0.4714\n    Epoch 7, Train Loss: 0.6845226177147457, Validation Accuracy: 0.4143\n    Epoch 8, Train Loss: 0.6855286581175668, Validation Accuracy: 0.4143\n    Epoch 9, Train Loss: 0.6812949461596353, Validation Accuracy: 0.4857\n    Epoch 10, Train Loss: 0.6769425528390067, Validation Accuracy: 0.5214\n    Epoch 11, Train Loss: 0.6783791840076446, Validation Accuracy: 0.4929\n    Epoch 12, Train Loss: 0.6741297330175128, Validation Accuracy: 0.4643\n    Epoch 13, Train Loss: 0.6671132956232343, Validation Accuracy: 0.4643\n    Epoch 14, Train Loss: 0.6627311910901751, Validation Accuracy: 0.4643\n    Epoch 15, Train Loss: 0.688248028925487, Validation Accuracy: 0.4786\n    Epoch 16, Train Loss: 0.6789343799863543, Validation Accuracy: 0.3929\n    Epoch 17, Train Loss: 0.6770548939704895, Validation Accuracy: 0.4286\n    Epoch 18, Train Loss: 0.6747145899704524, Validation Accuracy: 0.4429\n    Epoch 19, Train Loss: 0.6716752333300454, Validation Accuracy: 0.5000\n    Epoch 20, Train Loss: 0.6679088847977774, Validation Accuracy: 0.3929\n    Epoch 21, Train Loss: 0.6675338055406298, Validation Accuracy: 0.4357\n    Epoch 22, Train Loss: 0.6621674060821533, Validation Accuracy: 0.4500\n    Epoch 23, Train Loss: 0.6572071194648743, Validation Accuracy: 0.5286\n    Epoch 24, Train Loss: 0.6601680934429168, Validation Accuracy: 0.5143\n    Epoch 25, Train Loss: 0.6533683657646179, Validation Accuracy: 0.4929\n    Epoch 26, Train Loss: 0.6483252338000707, Validation Accuracy: 0.5143\n    Epoch 27, Train Loss: 0.640364893419402, Validation Accuracy: 0.5214\n    Epoch 28, Train Loss: 0.6574154683521816, Validation Accuracy: 0.5000\n    Epoch 29, Train Loss: 0.6478065703596387, Validation Accuracy: 0.4000\n    Epoch 30, Train Loss: 0.6471080435173852, Validation Accuracy: 0.4071\n    Epoch 31, Train Loss: 0.6356554486921855, Validation Accuracy: 0.4929\n    Epoch 32, Train Loss: 0.6346981759582248, Validation Accuracy: 0.5071\n    Epoch 33, Train Loss: 0.6295619134392058, Validation Accuracy: 0.4500\n    Epoch 34, Train Loss: 0.6311175652912685, Validation Accuracy: 0.4429\n    Epoch 35, Train Loss: 0.6293309697083065, Validation Accuracy: 0.4357\n    Epoch 36, Train Loss: 0.6203299113682338, Validation Accuracy: 0.4571\n    Epoch 37, Train Loss: 0.617344840509551, Validation Accuracy: 0.4500\n    Epoch 38, Train Loss: 0.6113125115633011, Validation Accuracy: 0.4786\n    Epoch 39, Train Loss: 0.6092043714863914, Validation Accuracy: 0.4429\n    Epoch 40, Train Loss: 0.6035737276077271, Validation Accuracy: 0.5143\n    Epoch 41, Train Loss: 0.6048109561204911, Validation Accuracy: 0.4643\n    Epoch 42, Train Loss: 0.5942480593919754, Validation Accuracy: 0.4929\n    Epoch 43, Train Loss: 0.5874386689492634, Validation Accuracy: 0.5286\n    Epoch 44, Train Loss: 0.589412818636213, Validation Accuracy: 0.4643\n    Epoch 45, Train Loss: 0.576792340193476, Validation Accuracy: 0.5000\n    Epoch 46, Train Loss: 0.5766855133431298, Validation Accuracy: 0.5000\n    Epoch 47, Train Loss: 0.5629069464547294, Validation Accuracy: 0.5000\n    Epoch 48, Train Loss: 0.5652359021561486, Validation Accuracy: 0.4929\n    Epoch 49, Train Loss: 0.5593742587736674, Validation Accuracy: 0.4929\n    Epoch 50, Train Loss: 0.5531061742986951, Validation Accuracy: 0.5000\nEvaluating on Test Data...\n\n    Accuracy: 0.5400\n    Precision: 0.5405\n    Recall: 0.5400\n    F1-Score: 0.5387\n    ROC AUC: 0.5400\nTraining with Batch=8, LR=0.001, Optimizer=SGD\n    Epoch 1, Train Loss: 0.6940724756036486, Validation Accuracy: 0.4500\n    Epoch 2, Train Loss: 0.693967683826174, Validation Accuracy: 0.4500\n    Epoch 3, Train Loss: 0.6938705103737968, Validation Accuracy: 0.4500\n    Epoch 4, Train Loss: 0.6937961867877416, Validation Accuracy: 0.4500\n    Epoch 5, Train Loss: 0.6937204710074834, Validation Accuracy: 0.4500\n    Epoch 6, Train Loss: 0.6936704576015472, Validation Accuracy: 0.4500\n    Epoch 7, Train Loss: 0.6935929213251386, Validation Accuracy: 0.4500\n    Epoch 8, Train Loss: 0.6935517617634365, Validation Accuracy: 0.4500\n    Epoch 9, Train Loss: 0.6934996034417834, Validation Accuracy: 0.4500\n    Epoch 10, Train Loss: 0.6934484243392944, Validation Accuracy: 0.4500\n    Epoch 11, Train Loss: 0.6934113306658608, Validation Accuracy: 0.4500\n    Epoch 12, Train Loss: 0.6933678959097181, Validation Accuracy: 0.4500\n    Epoch 13, Train Loss: 0.6933495104312897, Validation Accuracy: 0.4500\n    Epoch 14, Train Loss: 0.6933232026440757, Validation Accuracy: 0.4500\n    Epoch 15, Train Loss: 0.6932967688356128, Validation Accuracy: 0.4500\n    Epoch 16, Train Loss: 0.6932648020131248, Validation Accuracy: 0.4500\n    Epoch 17, Train Loss: 0.6932403343064445, Validation Accuracy: 0.4500\n    Epoch 18, Train Loss: 0.6932420458112444, Validation Accuracy: 0.4500\n    Epoch 19, Train Loss: 0.6932047571454729, Validation Accuracy: 0.4500\n    Epoch 20, Train Loss: 0.6932023687022073, Validation Accuracy: 0.4500\n    Epoch 21, Train Loss: 0.6931667481149946, Validation Accuracy: 0.4500\n    Epoch 22, Train Loss: 0.6931566553456443, Validation Accuracy: 0.4500\n    Epoch 23, Train Loss: 0.6931484861033304, Validation Accuracy: 0.4500\n    Epoch 24, Train Loss: 0.693115018095289, Validation Accuracy: 0.4500\n    Epoch 25, Train Loss: 0.6931225112506322, Validation Accuracy: 0.4500\n    Epoch 26, Train Loss: 0.6931137621402741, Validation Accuracy: 0.4500\n    Epoch 27, Train Loss: 0.6930945634841919, Validation Accuracy: 0.4500\n    Epoch 28, Train Loss: 0.6930876255035401, Validation Accuracy: 0.4500\n    Epoch 29, Train Loss: 0.6930752175194876, Validation Accuracy: 0.4500\n    Epoch 30, Train Loss: 0.693075795684542, Validation Accuracy: 0.4500\n    Epoch 31, Train Loss: 0.6930733161313193, Validation Accuracy: 0.4500\n    Epoch 32, Train Loss: 0.6930517605372838, Validation Accuracy: 0.4500\n    Epoch 33, Train Loss: 0.6930402295930045, Validation Accuracy: 0.4500\n    Epoch 34, Train Loss: 0.6930429399013519, Validation Accuracy: 0.4500\n    Epoch 35, Train Loss: 0.6930295390742166, Validation Accuracy: 0.4500\n    Epoch 36, Train Loss: 0.6930235258170536, Validation Accuracy: 0.4500\n    Epoch 37, Train Loss: 0.6930385231971741, Validation Accuracy: 0.4500\n    Epoch 38, Train Loss: 0.6930170638220651, Validation Accuracy: 0.4500\n    Epoch 39, Train Loss: 0.6930073891367231, Validation Accuracy: 0.4500\n    Epoch 40, Train Loss: 0.6930269019944327, Validation Accuracy: 0.4500\n    Epoch 41, Train Loss: 0.6930177390575409, Validation Accuracy: 0.4500\n    Epoch 42, Train Loss: 0.6929940734590803, Validation Accuracy: 0.4500\n    Epoch 43, Train Loss: 0.6930015427725655, Validation Accuracy: 0.4500\n    Epoch 44, Train Loss: 0.6929867173944201, Validation Accuracy: 0.4500\n    Epoch 45, Train Loss: 0.6929897436073849, Validation Accuracy: 0.4500\n    Epoch 46, Train Loss: 0.6929903566837311, Validation Accuracy: 0.4500\n    Epoch 47, Train Loss: 0.692972834621157, Validation Accuracy: 0.4500\n    Epoch 48, Train Loss: 0.6929714134761266, Validation Accuracy: 0.4500\n    Epoch 49, Train Loss: 0.6929670453071595, Validation Accuracy: 0.4500\n    Epoch 50, Train Loss: 0.6929557187216623, Validation Accuracy: 0.4500\nEvaluating on Test Data...\n\n    Accuracy: 0.5000\n    Precision: 0.2500\n    Recall: 0.5000\n    F1-Score: 0.3333\n    ROC AUC: 0.5000\nTraining with Batch=8, LR=0.0001, Optimizer=Adam\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"    Epoch 1, Train Loss: 0.697437356199537, Validation Accuracy: 0.5500\n    Epoch 2, Train Loss: 0.6953727619988578, Validation Accuracy: 0.5500\n    Epoch 3, Train Loss: 0.694012508222035, Validation Accuracy: 0.5429\n    Epoch 4, Train Loss: 0.6932177654334477, Validation Accuracy: 0.5643\n    Epoch 5, Train Loss: 0.6926455463681902, Validation Accuracy: 0.5429\n    Epoch 6, Train Loss: 0.6925046001161848, Validation Accuracy: 0.4071\n    Epoch 7, Train Loss: 0.6923880704811641, Validation Accuracy: 0.4214\n    Epoch 8, Train Loss: 0.6919628458363669, Validation Accuracy: 0.4214\n    Epoch 9, Train Loss: 0.6918787334646498, Validation Accuracy: 0.4143\n    Epoch 10, Train Loss: 0.6921291376863207, Validation Accuracy: 0.4214\n    Epoch 11, Train Loss: 0.69185066478593, Validation Accuracy: 0.4214\n    Epoch 12, Train Loss: 0.6915016225406102, Validation Accuracy: 0.4429\n    Epoch 13, Train Loss: 0.69165033357484, Validation Accuracy: 0.4143\n    Epoch 14, Train Loss: 0.6915948365415846, Validation Accuracy: 0.4214\n    Epoch 15, Train Loss: 0.6912671966212136, Validation Accuracy: 0.4143\n    Epoch 16, Train Loss: 0.6909938991069794, Validation Accuracy: 0.4214\n    Epoch 17, Train Loss: 0.6909505418368749, Validation Accuracy: 0.4143\n    Epoch 18, Train Loss: 0.6907959793295179, Validation Accuracy: 0.4286\n    Epoch 19, Train Loss: 0.6907249970095498, Validation Accuracy: 0.4286\n    Epoch 20, Train Loss: 0.6908103900296347, Validation Accuracy: 0.4071\n    Epoch 21, Train Loss: 0.690475993497031, Validation Accuracy: 0.4143\n    Epoch 22, Train Loss: 0.6902072778769902, Validation Accuracy: 0.4214\n    Epoch 23, Train Loss: 0.6899633620466504, Validation Accuracy: 0.4357\n    Epoch 24, Train Loss: 0.6896974631718227, Validation Accuracy: 0.4143\n    Epoch 25, Train Loss: 0.6893394955566952, Validation Accuracy: 0.4214\n    Epoch 26, Train Loss: 0.6892638989857265, Validation Accuracy: 0.4286\n    Epoch 27, Train Loss: 0.6892994863646371, Validation Accuracy: 0.4286\n    Epoch 28, Train Loss: 0.6887343457766941, Validation Accuracy: 0.4429\n    Epoch 29, Train Loss: 0.6886773696967534, Validation Accuracy: 0.4429\n    Epoch 30, Train Loss: 0.6881107687950134, Validation Accuracy: 0.4214\n    Epoch 31, Train Loss: 0.6881835699081421, Validation Accuracy: 0.4643\n    Epoch 32, Train Loss: 0.687547174521855, Validation Accuracy: 0.4786\n    Epoch 33, Train Loss: 0.6870567858219147, Validation Accuracy: 0.4357\n    Epoch 34, Train Loss: 0.6868055828980038, Validation Accuracy: 0.4786\n    Epoch 35, Train Loss: 0.6862866180283683, Validation Accuracy: 0.4786\n    Epoch 36, Train Loss: 0.6866034473691668, Validation Accuracy: 0.4571\n    Epoch 37, Train Loss: 0.6857717224529811, Validation Accuracy: 0.4786\n    Epoch 38, Train Loss: 0.6856163936001914, Validation Accuracy: 0.4929\n    Epoch 39, Train Loss: 0.6847030946186611, Validation Accuracy: 0.4929\n    Epoch 40, Train Loss: 0.6841859102249146, Validation Accuracy: 0.5000\n    Epoch 41, Train Loss: 0.6838425448962621, Validation Accuracy: 0.5214\n    Epoch 42, Train Loss: 0.6834148108959198, Validation Accuracy: 0.5000\n    Epoch 43, Train Loss: 0.6829395490033286, Validation Accuracy: 0.5214\n    Epoch 44, Train Loss: 0.682335501909256, Validation Accuracy: 0.5214\n    Epoch 45, Train Loss: 0.682044437953404, Validation Accuracy: 0.5143\n    Epoch 46, Train Loss: 0.680930049078805, Validation Accuracy: 0.5143\n    Epoch 47, Train Loss: 0.6805640297276633, Validation Accuracy: 0.5214\n    Epoch 48, Train Loss: 0.6809328019618988, Validation Accuracy: 0.5143\n    Epoch 49, Train Loss: 0.6800252837794167, Validation Accuracy: 0.5286\n    Epoch 50, Train Loss: 0.679267429453986, Validation Accuracy: 0.4929\nEvaluating on Test Data...\n\n    Accuracy: 0.4933\n    Precision: 0.4909\n    Recall: 0.4933\n    F1-Score: 0.4566\n    ROC AUC: 0.4933\nTraining with Batch=8, LR=0.0001, Optimizer=SGD\n    Epoch 1, Train Loss: 0.6967111476830073, Validation Accuracy: 0.5500\n    Epoch 2, Train Loss: 0.6966606829847608, Validation Accuracy: 0.5500\n    Epoch 3, Train Loss: 0.6966117143630981, Validation Accuracy: 0.5500\n    Epoch 4, Train Loss: 0.696562626532146, Validation Accuracy: 0.5500\n    Epoch 5, Train Loss: 0.6965123432023185, Validation Accuracy: 0.5500\n    Epoch 6, Train Loss: 0.6964645343167442, Validation Accuracy: 0.5500\n    Epoch 7, Train Loss: 0.6964176484516689, Validation Accuracy: 0.5500\n    Epoch 8, Train Loss: 0.6963712240968432, Validation Accuracy: 0.5500\n    Epoch 9, Train Loss: 0.696323721749442, Validation Accuracy: 0.5500\n    Epoch 10, Train Loss: 0.696279798235212, Validation Accuracy: 0.5500\n    Epoch 11, Train Loss: 0.6962343198912484, Validation Accuracy: 0.5500\n    Epoch 12, Train Loss: 0.6961904559816633, Validation Accuracy: 0.5500\n    Epoch 13, Train Loss: 0.6961459875106811, Validation Accuracy: 0.5500\n    Epoch 14, Train Loss: 0.696102660042899, Validation Accuracy: 0.5500\n    Epoch 15, Train Loss: 0.6960593180997031, Validation Accuracy: 0.5500\n    Epoch 16, Train Loss: 0.6960167569773538, Validation Accuracy: 0.5500\n    Epoch 17, Train Loss: 0.6959754177502223, Validation Accuracy: 0.5500\n    Epoch 18, Train Loss: 0.6959328762122563, Validation Accuracy: 0.5500\n    Epoch 19, Train Loss: 0.6958916102136884, Validation Accuracy: 0.5500\n    Epoch 20, Train Loss: 0.6958511429173606, Validation Accuracy: 0.5500\n    Epoch 21, Train Loss: 0.6958125012261527, Validation Accuracy: 0.5500\n    Epoch 22, Train Loss: 0.6957721744264875, Validation Accuracy: 0.5500\n    Epoch 23, Train Loss: 0.6957362421921321, Validation Accuracy: 0.5500\n    Epoch 24, Train Loss: 0.6956953133855547, Validation Accuracy: 0.5500\n    Epoch 25, Train Loss: 0.6956590286323002, Validation Accuracy: 0.5500\n    Epoch 26, Train Loss: 0.6956205521311078, Validation Accuracy: 0.5500\n    Epoch 27, Train Loss: 0.6955844359738487, Validation Accuracy: 0.5500\n    Epoch 28, Train Loss: 0.6955477901867457, Validation Accuracy: 0.5500\n    Epoch 29, Train Loss: 0.6955115190574102, Validation Accuracy: 0.5500\n    Epoch 30, Train Loss: 0.6954752683639527, Validation Accuracy: 0.5500\n    Epoch 31, Train Loss: 0.6954407802649907, Validation Accuracy: 0.5500\n    Epoch 32, Train Loss: 0.6954064284052167, Validation Accuracy: 0.5500\n    Epoch 33, Train Loss: 0.6953711339405605, Validation Accuracy: 0.5500\n    Epoch 34, Train Loss: 0.6953364798000881, Validation Accuracy: 0.5500\n    Epoch 35, Train Loss: 0.6953043750354222, Validation Accuracy: 0.5500\n    Epoch 36, Train Loss: 0.6952698375497546, Validation Accuracy: 0.5500\n    Epoch 37, Train Loss: 0.695238425901958, Validation Accuracy: 0.5500\n    Epoch 38, Train Loss: 0.6952073463371822, Validation Accuracy: 0.5500\n    Epoch 39, Train Loss: 0.6951760292053223, Validation Accuracy: 0.5500\n    Epoch 40, Train Loss: 0.695143290076937, Validation Accuracy: 0.5500\n    Epoch 41, Train Loss: 0.6951124429702759, Validation Accuracy: 0.5500\n    Epoch 42, Train Loss: 0.6950821323054177, Validation Accuracy: 0.5500\n    Epoch 43, Train Loss: 0.6950514835970742, Validation Accuracy: 0.5500\n    Epoch 44, Train Loss: 0.6950222824301039, Validation Accuracy: 0.5500\n    Epoch 45, Train Loss: 0.6949927202292852, Validation Accuracy: 0.5500\n    Epoch 46, Train Loss: 0.6949652808053153, Validation Accuracy: 0.5500\n    Epoch 47, Train Loss: 0.6949325135775974, Validation Accuracy: 0.5500\n    Epoch 48, Train Loss: 0.6949049345084599, Validation Accuracy: 0.5500\n    Epoch 49, Train Loss: 0.6948765499251229, Validation Accuracy: 0.5500\n    Epoch 50, Train Loss: 0.6948502625737871, Validation Accuracy: 0.5500\nEvaluating on Test Data...\n\n    Accuracy: 0.5000\n    Precision: 0.2500\n    Recall: 0.5000\n    F1-Score: 0.3333\n    ROC AUC: 0.5000\nTraining with Batch=64, LR=0.001, Optimizer=Adam\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"    Epoch 1, Train Loss: 0.6941679848564996, Validation Accuracy: 0.4500\n    Epoch 2, Train Loss: 0.6927390231026543, Validation Accuracy: 0.4500\n    Epoch 3, Train Loss: 0.6925112803777059, Validation Accuracy: 0.4714\n    Epoch 4, Train Loss: 0.6923618581559923, Validation Accuracy: 0.4786\n    Epoch 5, Train Loss: 0.6924170123206245, Validation Accuracy: 0.4643\n    Epoch 6, Train Loss: 0.6919620831807455, Validation Accuracy: 0.4714\n    Epoch 7, Train Loss: 0.6914607021543715, Validation Accuracy: 0.4214\n    Epoch 8, Train Loss: 0.6917296184433831, Validation Accuracy: 0.4714\n    Epoch 9, Train Loss: 0.6908352904849582, Validation Accuracy: 0.4786\n    Epoch 10, Train Loss: 0.6904791328642104, Validation Accuracy: 0.5000\n    Epoch 11, Train Loss: 0.6884550915824043, Validation Accuracy: 0.4857\n    Epoch 12, Train Loss: 0.6881401472621493, Validation Accuracy: 0.5429\n    Epoch 13, Train Loss: 0.6903587910864089, Validation Accuracy: 0.5000\n    Epoch 14, Train Loss: 0.682135197851393, Validation Accuracy: 0.5000\n    Epoch 15, Train Loss: 0.6770345767339071, Validation Accuracy: 0.5286\n    Epoch 16, Train Loss: 0.6909717122713724, Validation Accuracy: 0.5500\n    Epoch 17, Train Loss: 0.6895553602112664, Validation Accuracy: 0.5643\n    Epoch 18, Train Loss: 0.6871215303738912, Validation Accuracy: 0.4500\n    Epoch 19, Train Loss: 0.6869416236877441, Validation Accuracy: 0.4786\n    Epoch 20, Train Loss: 0.6867340008417765, Validation Accuracy: 0.4500\n    Epoch 21, Train Loss: 0.6857666903071933, Validation Accuracy: 0.4214\n    Epoch 22, Train Loss: 0.6852032144864401, Validation Accuracy: 0.4714\n    Epoch 23, Train Loss: 0.684014293882582, Validation Accuracy: 0.4714\n    Epoch 24, Train Loss: 0.6837459802627563, Validation Accuracy: 0.4643\n    Epoch 25, Train Loss: 0.6825683183140225, Validation Accuracy: 0.4571\n    Epoch 26, Train Loss: 0.6808315250608656, Validation Accuracy: 0.5357\n    Epoch 27, Train Loss: 0.6797932849989997, Validation Accuracy: 0.5429\n    Epoch 28, Train Loss: 0.6795644495222304, Validation Accuracy: 0.5000\n    Epoch 29, Train Loss: 0.6769925024774339, Validation Accuracy: 0.5429\n    Epoch 30, Train Loss: 0.6751999457677206, Validation Accuracy: 0.5000\n    Epoch 31, Train Loss: 0.6733993159400092, Validation Accuracy: 0.5214\n    Epoch 32, Train Loss: 0.6757171418931749, Validation Accuracy: 0.5071\n    Epoch 33, Train Loss: 0.6715078817473518, Validation Accuracy: 0.4929\n    Epoch 34, Train Loss: 0.6655707624223497, Validation Accuracy: 0.5071\n    Epoch 35, Train Loss: 0.6647426883379618, Validation Accuracy: 0.5214\n    Epoch 36, Train Loss: 0.6723549365997314, Validation Accuracy: 0.5071\n    Epoch 37, Train Loss: 0.6675535175535414, Validation Accuracy: 0.4571\n    Epoch 38, Train Loss: 0.6659438278939989, Validation Accuracy: 0.4643\n    Epoch 39, Train Loss: 0.6539029743936327, Validation Accuracy: 0.5286\n    Epoch 40, Train Loss: 0.6713403529591031, Validation Accuracy: 0.5786\n    Epoch 41, Train Loss: 0.6674342155456543, Validation Accuracy: 0.5286\n    Epoch 42, Train Loss: 0.6591070294380188, Validation Accuracy: 0.5143\n    Epoch 43, Train Loss: 0.6564903722869025, Validation Accuracy: 0.5071\n    Epoch 44, Train Loss: 0.6528441972202725, Validation Accuracy: 0.5000\n    Epoch 45, Train Loss: 0.6492042872640822, Validation Accuracy: 0.4929\n    Epoch 46, Train Loss: 0.6463169521755643, Validation Accuracy: 0.4786\n    Epoch 47, Train Loss: 0.644144184059567, Validation Accuracy: 0.4714\n    Epoch 48, Train Loss: 0.6440266569455465, Validation Accuracy: 0.4786\n    Epoch 49, Train Loss: 0.6384576823976305, Validation Accuracy: 0.5071\n    Epoch 50, Train Loss: 0.6384346683820089, Validation Accuracy: 0.4857\nEvaluating on Test Data...\n\n    Accuracy: 0.5000\n    Precision: 0.5000\n    Recall: 0.5000\n    F1-Score: 0.4637\n    ROC AUC: 0.5000\nTraining with Batch=64, LR=0.001, Optimizer=SGD\n    Epoch 1, Train Loss: 0.6933629446559482, Validation Accuracy: 0.4500\n    Epoch 2, Train Loss: 0.6931678586535983, Validation Accuracy: 0.4500\n    Epoch 3, Train Loss: 0.692984229988522, Validation Accuracy: 0.4500\n    Epoch 4, Train Loss: 0.6931968530019125, Validation Accuracy: 0.4500\n    Epoch 5, Train Loss: 0.6927707327736748, Validation Accuracy: 0.4500\n    Epoch 6, Train Loss: 0.6933029161559211, Validation Accuracy: 0.4500\n    Epoch 7, Train Loss: 0.6929929256439209, Validation Accuracy: 0.4500\n    Epoch 8, Train Loss: 0.6929564078648885, Validation Accuracy: 0.4500\n    Epoch 9, Train Loss: 0.6928328805499606, Validation Accuracy: 0.4500\n    Epoch 10, Train Loss: 0.6927354865603976, Validation Accuracy: 0.4500\n    Epoch 11, Train Loss: 0.6930336952209473, Validation Accuracy: 0.4500\n    Epoch 12, Train Loss: 0.6928361654281616, Validation Accuracy: 0.4500\n    Epoch 13, Train Loss: 0.6931125455432467, Validation Accuracy: 0.4500\n    Epoch 14, Train Loss: 0.6931630969047546, Validation Accuracy: 0.4500\n    Epoch 15, Train Loss: 0.6933863096767001, Validation Accuracy: 0.4500\n    Epoch 16, Train Loss: 0.6934808890024821, Validation Accuracy: 0.4500\n    Epoch 17, Train Loss: 0.6929082539346483, Validation Accuracy: 0.4500\n    Epoch 18, Train Loss: 0.6929016510645548, Validation Accuracy: 0.4500\n    Epoch 19, Train Loss: 0.6927710506651137, Validation Accuracy: 0.4500\n    Epoch 20, Train Loss: 0.6928458015124003, Validation Accuracy: 0.4500\n    Epoch 21, Train Loss: 0.6930260724491544, Validation Accuracy: 0.4500\n    Epoch 22, Train Loss: 0.6931808259752061, Validation Accuracy: 0.4500\n    Epoch 23, Train Loss: 0.6930122044351366, Validation Accuracy: 0.4500\n    Epoch 24, Train Loss: 0.6931829651196798, Validation Accuracy: 0.4500\n    Epoch 25, Train Loss: 0.6929896738794115, Validation Accuracy: 0.4500\n    Epoch 26, Train Loss: 0.6930404835277133, Validation Accuracy: 0.4500\n    Epoch 27, Train Loss: 0.6930514971415201, Validation Accuracy: 0.4500\n    Epoch 28, Train Loss: 0.6931622624397278, Validation Accuracy: 0.4500\n    Epoch 29, Train Loss: 0.6928634643554688, Validation Accuracy: 0.4500\n    Epoch 30, Train Loss: 0.6928106546401978, Validation Accuracy: 0.4500\n    Epoch 31, Train Loss: 0.6929356323348151, Validation Accuracy: 0.4500\n    Epoch 32, Train Loss: 0.6931974291801453, Validation Accuracy: 0.4500\n    Epoch 33, Train Loss: 0.6931359767913818, Validation Accuracy: 0.4500\n    Epoch 34, Train Loss: 0.6928800410694547, Validation Accuracy: 0.4500\n    Epoch 35, Train Loss: 0.6929843028386434, Validation Accuracy: 0.4500\n    Epoch 36, Train Loss: 0.6929146846135458, Validation Accuracy: 0.4500\n    Epoch 37, Train Loss: 0.6930190655920241, Validation Accuracy: 0.4500\n    Epoch 38, Train Loss: 0.6928040583928426, Validation Accuracy: 0.4500\n    Epoch 39, Train Loss: 0.6926808291011386, Validation Accuracy: 0.4500\n    Epoch 40, Train Loss: 0.6929382814301385, Validation Accuracy: 0.4500\n    Epoch 41, Train Loss: 0.6928140719731649, Validation Accuracy: 0.4500\n    Epoch 42, Train Loss: 0.6930404239230685, Validation Accuracy: 0.4500\n    Epoch 43, Train Loss: 0.6929121481047736, Validation Accuracy: 0.4500\n    Epoch 44, Train Loss: 0.6929330560896132, Validation Accuracy: 0.4500\n    Epoch 45, Train Loss: 0.6931062671873305, Validation Accuracy: 0.4500\n    Epoch 46, Train Loss: 0.6930382450421652, Validation Accuracy: 0.4500\n    Epoch 47, Train Loss: 0.6930306090248955, Validation Accuracy: 0.4500\n    Epoch 48, Train Loss: 0.6927193535698785, Validation Accuracy: 0.4500\n    Epoch 49, Train Loss: 0.6929607457584805, Validation Accuracy: 0.4500\n    Epoch 50, Train Loss: 0.6927776336669922, Validation Accuracy: 0.4500\nEvaluating on Test Data...\n\n    Accuracy: 0.5000\n    Precision: 0.2500\n    Recall: 0.5000\n    F1-Score: 0.3333\n    ROC AUC: 0.5000\nTraining with Batch=64, LR=0.0001, Optimizer=Adam\n    Epoch 1, Train Loss: 0.6941883762677511, Validation Accuracy: 0.4500\n    Epoch 2, Train Loss: 0.6934653917948405, Validation Accuracy: 0.4500\n    Epoch 3, Train Loss: 0.6935215658611722, Validation Accuracy: 0.4500\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"    Epoch 4, Train Loss: 0.6935436725616455, Validation Accuracy: 0.4500\n    Epoch 5, Train Loss: 0.693242245250278, Validation Accuracy: 0.4500\n    Epoch 6, Train Loss: 0.6930011841985915, Validation Accuracy: 0.4500\n    Epoch 7, Train Loss: 0.6928467485639784, Validation Accuracy: 0.4500\n    Epoch 8, Train Loss: 0.6930952204598321, Validation Accuracy: 0.4500\n    Epoch 9, Train Loss: 0.6929164131482443, Validation Accuracy: 0.4500\n    Epoch 10, Train Loss: 0.6931674083073934, Validation Accuracy: 0.4500\n    Epoch 11, Train Loss: 0.6930251916249593, Validation Accuracy: 0.4500\n    Epoch 12, Train Loss: 0.6927636199527316, Validation Accuracy: 0.4500\n    Epoch 13, Train Loss: 0.6928004556232028, Validation Accuracy: 0.4500\n    Epoch 14, Train Loss: 0.6927384734153748, Validation Accuracy: 0.4500\n    Epoch 15, Train Loss: 0.6925275723139445, Validation Accuracy: 0.4500\n    Epoch 16, Train Loss: 0.6926796833674113, Validation Accuracy: 0.4500\n    Epoch 17, Train Loss: 0.6924329664972093, Validation Accuracy: 0.4500\n    Epoch 18, Train Loss: 0.6924790607558357, Validation Accuracy: 0.4500\n    Epoch 19, Train Loss: 0.6925422483020358, Validation Accuracy: 0.4500\n    Epoch 20, Train Loss: 0.6924918956226773, Validation Accuracy: 0.4500\n    Epoch 21, Train Loss: 0.6925342149204678, Validation Accuracy: 0.4500\n    Epoch 22, Train Loss: 0.6922626230451796, Validation Accuracy: 0.4500\n    Epoch 23, Train Loss: 0.692406588130527, Validation Accuracy: 0.4500\n    Epoch 24, Train Loss: 0.6924420131577386, Validation Accuracy: 0.4500\n    Epoch 25, Train Loss: 0.6922842330402799, Validation Accuracy: 0.4500\n    Epoch 26, Train Loss: 0.6923646132151285, Validation Accuracy: 0.4500\n    Epoch 27, Train Loss: 0.6924182441499498, Validation Accuracy: 0.4500\n    Epoch 28, Train Loss: 0.6922960811191134, Validation Accuracy: 0.4500\n    Epoch 29, Train Loss: 0.6922028793228997, Validation Accuracy: 0.4571\n    Epoch 30, Train Loss: 0.6920377016067505, Validation Accuracy: 0.4571\n    Epoch 31, Train Loss: 0.6921326120694479, Validation Accuracy: 0.4571\n    Epoch 32, Train Loss: 0.692020038763682, Validation Accuracy: 0.4571\n    Epoch 33, Train Loss: 0.6919668780432807, Validation Accuracy: 0.4571\n    Epoch 34, Train Loss: 0.6919853289922079, Validation Accuracy: 0.4429\n    Epoch 35, Train Loss: 0.691907975408766, Validation Accuracy: 0.4571\n    Epoch 36, Train Loss: 0.6919426454438103, Validation Accuracy: 0.4429\n    Epoch 37, Train Loss: 0.6918462779786851, Validation Accuracy: 0.4429\n    Epoch 38, Train Loss: 0.6917383140987821, Validation Accuracy: 0.4429\n    Epoch 39, Train Loss: 0.691921677854326, Validation Accuracy: 0.4429\n    Epoch 40, Train Loss: 0.6919678317175971, Validation Accuracy: 0.4357\n    Epoch 41, Train Loss: 0.6916642586390177, Validation Accuracy: 0.4500\n    Epoch 42, Train Loss: 0.691865603129069, Validation Accuracy: 0.4429\n    Epoch 43, Train Loss: 0.6916922463311089, Validation Accuracy: 0.4500\n    Epoch 44, Train Loss: 0.6915729708141751, Validation Accuracy: 0.4500\n    Epoch 45, Train Loss: 0.691708074675666, Validation Accuracy: 0.4500\n    Epoch 46, Train Loss: 0.6916440791553922, Validation Accuracy: 0.4500\n    Epoch 47, Train Loss: 0.6916077203220792, Validation Accuracy: 0.4500\n    Epoch 48, Train Loss: 0.6915232870313857, Validation Accuracy: 0.4714\n    Epoch 49, Train Loss: 0.6912077334192064, Validation Accuracy: 0.4714\n    Epoch 50, Train Loss: 0.6915041340721978, Validation Accuracy: 0.4500\nEvaluating on Test Data...\n\n    Accuracy: 0.5033\n    Precision: 0.5140\n    Recall: 0.5033\n    F1-Score: 0.3863\n    ROC AUC: 0.5033\nTraining with Batch=64, LR=0.0001, Optimizer=SGD\n    Epoch 1, Train Loss: 0.6932696368959215, Validation Accuracy: 0.4500\n    Epoch 2, Train Loss: 0.6932148602273729, Validation Accuracy: 0.4500\n    Epoch 3, Train Loss: 0.6932229399681091, Validation Accuracy: 0.4500\n    Epoch 4, Train Loss: 0.6932842665248447, Validation Accuracy: 0.4500\n    Epoch 5, Train Loss: 0.6932346489694383, Validation Accuracy: 0.4500\n    Epoch 6, Train Loss: 0.693290650844574, Validation Accuracy: 0.4500\n    Epoch 7, Train Loss: 0.6931907931963602, Validation Accuracy: 0.4500\n    Epoch 8, Train Loss: 0.6932848559485542, Validation Accuracy: 0.4500\n    Epoch 9, Train Loss: 0.6933538185225593, Validation Accuracy: 0.4500\n    Epoch 10, Train Loss: 0.6932345761193169, Validation Accuracy: 0.4500\n    Epoch 11, Train Loss: 0.6932336091995239, Validation Accuracy: 0.4500\n    Epoch 12, Train Loss: 0.6933524078792996, Validation Accuracy: 0.4500\n    Epoch 13, Train Loss: 0.6931380960676405, Validation Accuracy: 0.4500\n    Epoch 14, Train Loss: 0.6932547688484192, Validation Accuracy: 0.4500\n    Epoch 15, Train Loss: 0.6933459970686171, Validation Accuracy: 0.4500\n    Epoch 16, Train Loss: 0.6932629611757066, Validation Accuracy: 0.4500\n    Epoch 17, Train Loss: 0.6933187047640482, Validation Accuracy: 0.4500\n    Epoch 18, Train Loss: 0.6932253903812833, Validation Accuracy: 0.4500\n    Epoch 19, Train Loss: 0.6932369271914164, Validation Accuracy: 0.4500\n    Epoch 20, Train Loss: 0.6931820842954848, Validation Accuracy: 0.4500\n    Epoch 21, Train Loss: 0.6933975021044413, Validation Accuracy: 0.4500\n    Epoch 22, Train Loss: 0.6931903428501553, Validation Accuracy: 0.4500\n    Epoch 23, Train Loss: 0.6932239664925469, Validation Accuracy: 0.4500\n    Epoch 24, Train Loss: 0.693082696861691, Validation Accuracy: 0.4500\n    Epoch 25, Train Loss: 0.6933044592539469, Validation Accuracy: 0.4500\n    Epoch 26, Train Loss: 0.6933417320251465, Validation Accuracy: 0.4500\n    Epoch 27, Train Loss: 0.6932411127620273, Validation Accuracy: 0.4500\n    Epoch 28, Train Loss: 0.6932698819372389, Validation Accuracy: 0.4500\n    Epoch 29, Train Loss: 0.693270120355818, Validation Accuracy: 0.4500\n    Epoch 30, Train Loss: 0.6932265824741788, Validation Accuracy: 0.4500\n    Epoch 31, Train Loss: 0.6932029989030626, Validation Accuracy: 0.4500\n    Epoch 32, Train Loss: 0.6932767099804349, Validation Accuracy: 0.4500\n    Epoch 33, Train Loss: 0.6933134198188782, Validation Accuracy: 0.4500\n    Epoch 34, Train Loss: 0.6933198438750373, Validation Accuracy: 0.4500\n    Epoch 35, Train Loss: 0.693093823062049, Validation Accuracy: 0.4500\n    Epoch 36, Train Loss: 0.6931902898682488, Validation Accuracy: 0.4500\n    Epoch 37, Train Loss: 0.6933508184221056, Validation Accuracy: 0.4500\n    Epoch 38, Train Loss: 0.6933537721633911, Validation Accuracy: 0.4500\n    Epoch 39, Train Loss: 0.6932792398664687, Validation Accuracy: 0.4500\n    Epoch 40, Train Loss: 0.6932384636667039, Validation Accuracy: 0.4500\n    Epoch 41, Train Loss: 0.6932943595780267, Validation Accuracy: 0.4500\n    Epoch 42, Train Loss: 0.6933508316675822, Validation Accuracy: 0.4500\n    Epoch 43, Train Loss: 0.6932621399561564, Validation Accuracy: 0.4500\n    Epoch 44, Train Loss: 0.6933688256475661, Validation Accuracy: 0.4500\n    Epoch 45, Train Loss: 0.6933184795909457, Validation Accuracy: 0.4500\n    Epoch 46, Train Loss: 0.6931397053930495, Validation Accuracy: 0.4500\n    Epoch 47, Train Loss: 0.6932195954852634, Validation Accuracy: 0.4500\n    Epoch 48, Train Loss: 0.6932296090655856, Validation Accuracy: 0.4500\n    Epoch 49, Train Loss: 0.6932781140009562, Validation Accuracy: 0.4500\n    Epoch 50, Train Loss: 0.6932991478178236, Validation Accuracy: 0.4500\nEvaluating on Test Data...\n\n    Accuracy: 0.5033\n    Precision: 0.7508\n    Recall: 0.5033\n    F1-Score: 0.3407\n    ROC AUC: 0.5033\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"best_model = CNNLSTM(input_dim=120, hidden_dim=64, num_classes=len(y.unique())).to(device)\nbest_model.load_state_dict(best_model_state)\n_ = evaluate_model(best_model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-18T07:51:02.611843Z","iopub.execute_input":"2024-12-18T07:51:02.612190Z","iopub.status.idle":"2024-12-18T07:51:02.633402Z","shell.execute_reply.started":"2024-12-18T07:51:02.612162Z","shell.execute_reply":"2024-12-18T07:51:02.632714Z"}},"outputs":[{"name":"stdout","text":"\n    Accuracy: 0.5400\n    Precision: 0.5405\n    Recall: 0.5400\n    F1-Score: 0.5387\n    ROC AUC: 0.5400\n","output_type":"stream"}],"execution_count":38}]}